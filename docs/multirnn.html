<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>Multi-layer RNN for Name Classification from Scratch&nbsp;|&nbsp;Notablog</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="Multi-layer RNN for Name Classification from Scratch">
  
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ“–&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="multirnn.html">
        <div class="Navbar__Btn">
          
          <span>Multi-layer RNN for Name Classification from Scratch</span>
        </div>
      </a>
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="data:image/svg+xml,&lt;svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22&gt;&lt;text text-anchor=%22middle%22 dominant-baseline=%22middle%22 x=%2250%22 y=%2255%22 font-size=%2280%22&gt;ðŸ˜€&lt;/text&gt;&lt;/svg&gt;"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
    
  
</nav>
  <header class="Header">
    
    <div class="Header__Spacer Header__Spacer--NoCover">
    </div>
    
    <h1 class="Header__Title">Multi-layer RNN for Name Classification from Scratch</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Sat, May 17, 2025</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--orange">
            <a href="tag/Deep Learning.html">Deep Learning</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/1f63c1134cdd808186cedcaafad25eb2" class="PageRoot"><div id="https://www.notion.so/1f63c1134cdd80168595ec40bee454b3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">This post is inspired by the PyTorch tutorial </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html">Classifying Names with a Character-Level RNN</a></span><span class="SemanticString">. However, we will go one step forward and build a multi-layer RNN, including defining the forward method explicitly as well. </span></span></p></div><div id="https://www.notion.so/1f63c1134cdd8012ad3ae2577fa4b1f5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd805eaecde0856bc50f02" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We start by setting up the available device as the default device to not bother with moving data&amp;model to device later on:</span></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80aba04af6cbb558b570" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">import</span> torch
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
<span class="token keyword">else</span><span class="token punctuation">:</span>
    device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>set_default_device<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd803aa2fce9254cae9ba3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80baa670d7ed65316596" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The names dataset can be reached through this downloadable </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://download.pytorch.org/tutorial/data.zip">link</a></span><span class="SemanticString">. Since there are many names from different languages and some characters are not available as ASCII chars, we are going to have this simple processing to make sure, we just deal with ASCII chars:</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd803f988bd792e79017a0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80148cd9dfa294e47095" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">import</span> string
<span class="token keyword">import</span> unicodedata

<span class="token comment"># We can use "_" to represent an out-of-vocabulary character, that is, any character we are not handling in our model</span>
allowed_characters <span class="token operator">=</span> string<span class="token punctuation">.</span>ascii_letters <span class="token operator">+</span> <span class="token string">" .,;'"</span> <span class="token operator">+</span> <span class="token string">"_"</span>
n_letters <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>allowed_characters<span class="token punctuation">)</span>

<span class="token comment"># Turn a Unicode string to plain ASCII, thanks to https://stackoverflow.com/a/518232/2809427</span>
<span class="token keyword">def</span> <span class="token function">unicodeToAscii</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token string">''</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>
        c <span class="token keyword">for</span> c <span class="token keyword">in</span> unicodedata<span class="token punctuation">.</span>normalize<span class="token punctuation">(</span><span class="token string">'NFD'</span><span class="token punctuation">,</span> s<span class="token punctuation">)</span>
        <span class="token keyword">if</span> unicodedata<span class="token punctuation">.</span>category<span class="token punctuation">(</span>c<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token string">'Mn'</span>
        <span class="token keyword">and</span> c <span class="token keyword">in</span> allowed_characters
    <span class="token punctuation">)</span></span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd80c29749ffc152f18070" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80b9a77be975a739d9c9" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We are going to process names as one-hot vectors, where indexes are based on the </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">allowed_characters</code></span></span></p></div><div id="https://www.notion.so/1f63c1134cdd803391efde982dfc151e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80e6ad5bf03087fefe97" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">def</span> <span class="token function">nametotensor</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
		<span class="token comment"># make sure name is ascii coded</span>
		name <span class="token operator">=</span> unicodeToAscii<span class="token punctuation">(</span>name<span class="token punctuation">)</span>
		zero_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>allowed_characters<span class="token punctuation">)</span><span class="token punctuation">)</span>
		tensor_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
		<span class="token comment"># we assume a name has at least two letters</span>
		<span class="token keyword">for</span> idx<span class="token punctuation">,</span> letter <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>
		letter_idx <span class="token operator">=</span> allowed_characters<span class="token punctuation">.</span>index<span class="token punctuation">(</span>letter<span class="token punctuation">)</span>
		letter_tensor <span class="token operator">=</span> zero_tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>
		letter_tensor<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>letter_idx<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>
		tensor_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>letter_tensor<span class="token punctuation">)</span>
		<span class="token keyword">return</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>tensor_list<span class="token punctuation">)</span></span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd80aa8562e221416fc798" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80378ed8fe1f964a4acf" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Now, weâ€™re ready to define the custom Dataset:</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd8053970cefb9b2ed59bb" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80029d56d168f5b08719" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset
<span class="token keyword">import</span> glob
<span class="token keyword">import</span> os

<span class="token keyword">class</span> <span class="token class-name">MyDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
	<span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_path<span class="token punctuation">)</span><span class="token punctuation">:</span>
		self<span class="token punctuation">.</span>data_path <span class="token operator">=</span> data_path
		self<span class="token punctuation">.</span>names <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
		self<span class="token punctuation">.</span>namestensors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
		self<span class="token punctuation">.</span>labels <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    file_names <span class="token operator">=</span> glob<span class="token punctuation">.</span>glob<span class="token punctuation">(</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>data_path<span class="token punctuation">,</span> <span class="token string">"*.txt"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> <span class="token builtin">file</span> <span class="token keyword">in</span> file_names<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{</span><span class="token builtin">file</span><span class="token punctuation">}</span></span><span class="token string"> processed"</span></span><span class="token punctuation">)</span>
        label <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>basename<span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"."</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        names <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token builtin">file</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"\\n"</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>names<span class="token punctuation">.</span>extend<span class="token punctuation">(</span>names<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>namestensors<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>nametotensor<span class="token punctuation">(</span>name<span class="token punctuation">)</span> <span class="token keyword">for</span> name <span class="token keyword">in</span> self<span class="token punctuation">.</span>names<span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>labels<span class="token punctuation">.</span>extend<span class="token punctuation">(</span><span class="token punctuation">[</span>label<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>names<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token comment"># get label tensors</span>
    self<span class="token punctuation">.</span>uniq_labels <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">set</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>labelstensors <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>uniq_labels<span class="token punctuation">.</span>index<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> self<span class="token punctuation">.</span>labels<span class="token punctuation">]</span>

<span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>names<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    name<span class="token punctuation">,</span> nametensor <span class="token operator">=</span> self<span class="token punctuation">.</span>names<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>namestensors<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    label<span class="token punctuation">,</span> labeltensor <span class="token operator">=</span> self<span class="token punctuation">.</span>labels<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>labelstensors<span class="token punctuation">[</span>idx<span class="token punctuation">]</span>
    <span class="token keyword">return</span> name<span class="token punctuation">,</span> label<span class="token punctuation">,</span> nametensor<span class="token punctuation">,</span> labeltensor</span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd80d0bd40e5f890a85fed" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/1f63c1134cdd80f4b5c1f696cf89f005" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/1f63c1134cdd80f4b5c1f696cf89f005"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">RNN Architecture </span></span></h1><div id="https://www.notion.so/1f63c1134cdd8000abf2ec7f4f9b30ba" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80dea618ee474bf12f04" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">RNNs are two neural networks stacked across layers and a sequence. While there are several ways to design its architecture, we are interested in a many-to-one relationships for given one-hot-encodings to end up with class probabilities. These stacked layers are recurred through two inputs: hidden states and input sequence. We can define the general form of the an RNN as follows:</span></span></p></div><p id="https://www.notion.so/1f63c1134cdd80ab8e07c27c05df975a" class="Equation" data-latex="H_t^l = \phi_t(H_t^{l-1}W_{ih} + b_{ih} + H_{t-1}^lW_{hh} + b_{hh})"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>t</mi><mi>l</mi></msubsup><mo>=</mo><msub><mi>Ï•</mi><mi>t</mi></msub><mo stretchy="false">(</mo><msubsup><mi>H</mi><mi>t</mi><mrow><mi>l</mi><mo>âˆ’</mo><mn>1</mn></mrow></msubsup><msub><mi>W</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>i</mi><mi>h</mi></mrow></msub><mo>+</mo><msubsup><mi>H</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow><mi>l</mi></msubsup><msub><mi>W</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo>+</mo><msub><mi>b</mi><mrow><mi>h</mi><mi>h</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H_t^l = \phi_t(H_t^{l-1}W_{ih} + b_{ih} + H_{t-1}^lW_{hh} + b_{hh})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.146108em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1491079999999998em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">Ï•</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999998em;"><span style="top:-2.4530000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.204439em;vertical-align:-0.305331em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.899108em;"><span style="top:-2.4530000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.1130000000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.305331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></p><div id="https://www.notion.so/1f63c1134cdd8084b0b6f66b1b18bed8" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">where </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="t "><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span></span></span><span class="SemanticString"> and </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="l "><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span></span></span></span></span></span><span class="SemanticString"> stands for the sequence and layer item; </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="\phi_t"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Ï•</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\phi_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">Ï•</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> for the activation function; </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="ih"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">ih</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">i</span><span class="mord mathdefault">h</span></span></span></span></span></span><span class="SemanticString"> and </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="hh "><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">hh </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">h</span><span class="mord mathdefault">h</span></span></span></span></span></span><span class="SemanticString"> for input-to-hidden and hidden-to-hidden. Note that </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="H_t^{l-1} = x[t] "><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>t</mi><mrow><mi>l</mi><mo>âˆ’</mo><mn>1</mn></mrow></msubsup><mo>=</mo><mi>x</mi><mo stretchy="false">[</mo><mi>t</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">H_t^{l-1} = x[t] </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.134995em;vertical-align:-0.24575599999999997em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8892389999999999em;"><span style="top:-2.454244em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.1031310000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.24575599999999997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">x</span><span class="mopen">[</span><span class="mord mathdefault">t</span><span class="mclose">]</span></span></span></span></span></span><span class="SemanticString"> for </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="l=0"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">l=0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span></span></span><span class="SemanticString">. Here we should remark some of the misleading differences between one-layer and multi-layer RNNs. The commonly used way to visualize an RNN is as follows:</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd8084b3edce3b821e6650" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80c88619fdc508b7e3f2" class="Image Image--PageWidth"><figure><a href="attachment:db39a089-b100-4a9c-b50c-fcb9049eb006:image.png?width=530"><img src="attachment:db39a089-b100-4a9c-b50c-fcb9049eb006:image.png?width=530" style="width:100%"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Figure 10.5 from Ian Goodfellow [1]</span></span></figcaption></figure></div><div id="https://www.notion.so/1f63c1134cdd802294fbc1b1f864b935" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The relation between hidden-to-hidden weights are based on the layer level, not the sequence (</span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="t"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.61508em;vertical-align:0em;"></span><span class="mord mathdefault">t</span></span></span></span></span></span><span class="SemanticString">) level which might be interpreted as such based on the given image. For instance, if we had 4 layers, the incoming hidden state for </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="H_t^3"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mi>t</mi><mn>3</mn></msubsup></mrow><annotation encoding="application/x-tex">H_t^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.061108em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-2.4530000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString"> would be </span><span class="SemanticString"><span class="SemanticString__Fragment SemanticString__Fragment--Math" data-latex="H_{t-1}^3"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>H</mi><mrow><mi>t</mi><mo>âˆ’</mo><mn>1</mn></mrow><mn>3</mn></msubsup></mrow><annotation encoding="application/x-tex">H_{t-1}^3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1205469999999997em;vertical-align:-0.30643899999999996em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999998em;"><span style="top:-2.451892em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mbin mtight">âˆ’</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.30643899999999996em;"><span></span></span></span></span></span></span></span></span></span></span></span><span class="SemanticString">.</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd804da2fbd17ddcdc653e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><h1 id="https://www.notion.so/1f63c1134cdd80b68fd6e7d84bcd19d7" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--1"><a class="Anchor" href="#https://www.notion.so/1f63c1134cdd80b68fd6e7d84bcd19d7"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">RNN Class</span></span></h1><div id="https://www.notion.so/1f63c1134cdd801d8f19cdc5b846e6f3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80d88bebef540ac87319" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">We define the multi-layer RNN class as follows. There are several points:</span></span></p></div><ul class="BulletedListWrapper"><li id="https://www.notion.so/1f63c1134cdd804291f1f50c88056ab3" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">We define weights as  </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">nn.Parameter</code></span><span class="SemanticString"> , which is not initialized by default so we use </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">xavier_transform</code></span><span class="SemanticString"> to initialize them.</span></span></li><li id="https://www.notion.so/1f63c1134cdd8058a33eebea55a30d20" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">A crucial part of the RNN class is how the forward method is handled. Based on the RNN equation given previously, one might be tempted to update hidden states using in-place operation such as :  </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">h[t] = tanh(h_[t] ...)</code></span><span class="SemanticString"> .  However, this would break the autograd function since hidden states are used during backpropagation and their leaf structure must not be changed. Thatâ€™s why we use a place-holder called </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">new_h_states</code></span><span class="SemanticString"> to update hidden states at once.</span></span></li><li id="https://www.notion.so/1f63c1134cdd808fbd22c9434375ea42" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString">Finally, we are only interested in the hidden state at the last layer of the last sequence.</span></span></li></ul><pre id="https://www.notion.so/1f63c1134cdd80dbb06ed56caf79f791" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parameter <span class="token keyword">import</span> Parameter
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> init

<span class="token keyword">class</span> <span class="token class-name">MyRNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> output_size<span class="token punctuation">,</span> num_layers <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>input_size <span class="token operator">=</span> input_size 
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>output_size <span class="token operator">=</span> output_size
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        
        <span class="token comment"># define weights and biases</span>
        self<span class="token punctuation">.</span>W_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_ih <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>W_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>b_hh <span class="token operator">=</span> Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>    

        <span class="token comment"># weight initialization</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># For the first layer, W_ih uses input_size</span>
            init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_ih<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># Slicing to match actual input_size</span>
            init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>b_ih<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
            init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>W_hh<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
            init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>b_hh<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>
            
        <span class="token comment"># output layers</span>
        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_size<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>LogSoftmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        seq_len<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        h_states <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seq_len<span class="token punctuation">)</span><span class="token punctuation">:</span>
            new_h_states <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
            <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># only the first layer takes the corresponding input</span>
                <span class="token keyword">if</span> layer <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                    new_h <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>x<span class="token punctuation">[</span>t<span class="token punctuation">]</span> @ self<span class="token punctuation">.</span>W_ih<span class="token punctuation">[</span>layer<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>self<span class="token punctuation">.</span>input_size<span class="token punctuation">]</span><span class="token punctuation">.</span>T <span class="token operator">+</span> self<span class="token punctuation">.</span>b_ih<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> <span class="token operator">+</span> h_states<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> @ self<span class="token punctuation">.</span>W_hh<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">.</span>T <span class="token operator">+</span> self<span class="token punctuation">.</span>b_hh<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">)</span>
                <span class="token keyword">else</span><span class="token punctuation">:</span>
                    new_h <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>new_h_states<span class="token punctuation">[</span>layer<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> @ self<span class="token punctuation">.</span>W_ih<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">.</span>T <span class="token operator">+</span> self<span class="token punctuation">.</span>b_ih<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> <span class="token operator">+</span> h_states<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> @ self<span class="token punctuation">.</span>W_hh<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">.</span>T <span class="token operator">+</span> self<span class="token punctuation">.</span>b_hh<span class="token punctuation">[</span>layer<span class="token punctuation">]</span> <span class="token punctuation">)</span>    
                new_h_states<span class="token punctuation">.</span>append<span class="token punctuation">(</span>new_h<span class="token punctuation">)</span>
            <span class="token comment"># we are interested in the output of the last layer </span>
            h_states <span class="token operator">=</span> new_h_states
            outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>new_h<span class="token punctuation">)</span>
            
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>h_states<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        log_softs <span class="token operator">=</span> self<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
        <span class="token keyword">return</span> outputs<span class="token punctuation">,</span> log_softs</span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd8050b9cce6f72100f32b" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">  </span></span></p></div><h2 id="https://www.notion.so/1f63c1134cdd80bcaddade6431696a17" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--2"><a class="Anchor" href="#https://www.notion.so/1f63c1134cdd80bcaddade6431696a17"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString">Training Process</span></span></h2><div id="https://www.notion.so/1f63c1134cdd8008b5a3e0f85b50fcaa" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80979bb0ddca5652b869" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The training function is as usual, with the notable exception of  </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">nn.utils.clip_grad_norm_</code></span><span class="SemanticString"> which clips the gradient of the RNN, that are especially prone to gradient underflow/overflow.</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd80e9addbd8d975bf3b04" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80698ea2d6e93a2bb806" class="Code Code--NoWrap"><code><span class="SemanticStringArray"><span class="SemanticString"><span><span class="token keyword">import</span> random 
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">training</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> training_data<span class="token punctuation">,</span> n_epochs<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token number">0.15</span><span class="token punctuation">,</span> show_freq<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    all_losses <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    all_probs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
    batch_losses <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        current_loss <span class="token operator">=</span> <span class="token number">0</span>
        batch_losses<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token comment"># make sure grad is zero at each epoch</span>
        model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        <span class="token comment"># we create batches based on indexes</span>
        indexes <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>training_data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        random<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>indexes<span class="token punctuation">)</span>
        batch_indexes <span class="token operator">=</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>indexes<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>indexes<span class="token punctuation">)</span> <span class="token operator">//</span> batch_size<span class="token punctuation">)</span>
        current_batch <span class="token operator">=</span> <span class="token number">1</span>
        <span class="token keyword">for</span> batch <span class="token keyword">in</span> batch_indexes<span class="token punctuation">:</span>
            batch_loss <span class="token operator">=</span> <span class="token number">0</span>
            <span class="token keyword">for</span> b_idx <span class="token keyword">in</span> batch<span class="token punctuation">:</span> <span class="token comment"># for each example in the current batch</span>
                label_tensor<span class="token punctuation">,</span> name_tensor<span class="token punctuation">,</span> label<span class="token punctuation">,</span> name <span class="token operator">=</span> training_data<span class="token punctuation">[</span>b_idx<span class="token punctuation">]</span>
                name_tensor<span class="token punctuation">,</span> label_tensor <span class="token operator">=</span> name_tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> label_tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
                outputs<span class="token punctuation">,</span> probs <span class="token operator">=</span> model<span class="token punctuation">(</span>name_tensor<span class="token punctuation">)</span>
                all_probs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>probs<span class="token punctuation">)</span>
                batch_loss <span class="token operator">+=</span> loss<span class="token punctuation">(</span>probs<span class="token punctuation">,</span> label_tensor<span class="token punctuation">)</span>
            batch_losses<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>append<span class="token punctuation">(</span>batch_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">)</span>
            current_loss <span class="token operator">+=</span> batch_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span>
            batch_loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token comment"># clip!</span>
            nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>clip_grad_norm_<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            
            <span class="token keyword">if</span> current_batch <span class="token operator">%</span> show_freq <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"batch </span><span class="token interpolation"><span class="token punctuation">{</span>current_batch<span class="token punctuation">}</span></span><span class="token string"> completed"</span></span><span class="token punctuation">)</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"current loss: </span><span class="token interpolation"><span class="token punctuation">{</span>batch_loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"predicted class:"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>probs<span class="token punctuation">)</span><span class="token punctuation">)</span>
            current_batch <span class="token operator">+=</span> <span class="token number">1</span>
        all_losses<span class="token punctuation">.</span>append<span class="token punctuation">(</span>current_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>batch_indexes<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> all_losses<span class="token punctuation">,</span> batch_losses
    </span></span></span></code></pre><div id="https://www.notion.so/1f63c1134cdd80cda042fb897d474b62" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd8000ad92d5e32e2cf35e" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">The resulting confusion matrix for the test data is found as follows. We also give the class distributions of the test data as a reference below.</span></span></p></div><div id="https://www.notion.so/1f63c1134cdd808f835bdfa84fae9ab0" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"></span></p></div><div id="https://www.notion.so/1f63c1134cdd80ff9bb0f8c5fbf7b124" class="Image Image--PageWidth"><figure><a href="attachment:d06a4d02-e6de-4f15-95f4-a13457c565ea:image.png?width=530"><img src="attachment:d06a4d02-e6de-4f15-95f4-a13457c565ea:image.png?width=530" style="width:100%"/></a><figcaption><span class="SemanticStringArray"><span class="SemanticString">Prediction accuracy across classes</span></span></figcaption></figure></div><div id="https://www.notion.so/1f63c1134cdd80e6bbebc7d59a7afb00" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Class distribution on the test set:</span></span></p></div><pre id="https://www.notion.so/1f63c1134cdd80e88383ca2f48a62dd7" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span>{&#x27;Russian&#x27;: 1881,
 &#x27;Scottish&#x27;: 20,
 &#x27;English&#x27;: 718,
 &#x27;French&#x27;: 46,
 &#x27;Czech&#x27;: 127,
 &#x27;Irish&#x27;: 32,
 &#x27;German&#x27;: 136,
 &#x27;Polish&#x27;: 29,
 &#x27;Arabic&#x27;: 401,
 &#x27;Dutch&#x27;: 74,
 &#x27;Italian&#x27;: 147,
 &#x27;Greek&#x27;: 44,
 &#x27;Korean&#x27;: 18,
 &#x27;Spanish&#x27;: 69,
 &#x27;Portuguese&#x27;: 14,
 &#x27;Japanese&#x27;: 192,
 &#x27;Vietnamese&#x27;: 17,
 &#x27;Chinese&#x27;: 49}</span></span></span></code></pre></article>
  <footer class="Footer">
  <div>&copy; Notablog 2024</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>

</body>

</html>